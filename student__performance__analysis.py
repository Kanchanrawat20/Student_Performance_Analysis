# -*- coding: utf-8 -*-
"""Student _performance _analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1miuWGM3A1dc6K5BRXVLekZj8WK-bNvt3
"""

# STEP 1: Data Collection
# Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# STEP 1.1: Read the dataset
df = pd.read_csv('/content/student_performance_analyze_dataset.csv')
df.head()  # Show first 5 rows of the dataset

#  STEP 2: Data Cleaning
# STEP 2.1: Check the shape of the dataset
print("Shape of dataset:", df.shape)

# STEP 2.2: Check basic info of dataset
df.info()

# STEP 2.3: Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# STEP 2.4: Check for duplicate rows
print("\nNumber of duplicate rows:", df.duplicated().sum())

# STEP 2.5: Drop duplicate rows if any
df = df.drop_duplicates()

# STEP 2.6: Handle missing values (here we drop them, but you can also fill them)
df = df.dropna()

# STEP 2.7: Re-check after cleaning
print("\nShape after cleaning:", df.shape)
df.head()

# Step 3: Data Preprocessing
from sklearn.preprocessing import LabelEncoder

# copy of original dataset
df_encoded = df.copy()

# Automatically encode all object-type columns as well as parental_level_of_education
le = LabelEncoder()
for col in df_encoded.columns:
    if df_encoded[col].dtype == 'object' or col == 'parental_level_of_education':
        df_encoded[col] = le.fit_transform(df_encoded[col])

# Step 3.1: Train-Test Split
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = df.select_dtypes(include=['int64', 'float64']).drop(columns=['math_score'])
y = df['math_score']

# Scale the features using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Now can split the data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Creating Final_Grade column from the average of three scores
df['Final_Grade'] = (df['math_score'] + df['reading_score'] + df['writing_score']) / 3

# STEP 4: Exploratory Data Analysis (EDA) Using Visualization
# STEP 4.1: Univariate Analysis
# Count of students by sports_participation
sns.countplot(data=df, x='sports_participation', palette='Set2', hue='sports_participation', legend=False)  # Or any desired color
plt.title("Sports Participation Among Students")
plt.xlabel("Participation")
plt.ylabel("Count")
plt.show()



# Count of students by gender
sns.countplot(data=df, x='gender', palette='Set2', hue='gender', legend=False)
plt.title("Gender Distribution")
plt.show()

# Count of students by lunch type
sns.countplot(data=df, x='lunch', palette='pastel', hue='lunch', legend=False)
plt.title("Lunch Type Distribution")
plt.show()

# STEP 4.1: Pie Chart for Extra Classes
extra_class_counts = df['extra_classes'].value_counts()
plt.figure(figsize=(6,6))
plt.pie(extra_class_counts, labels=extra_class_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))
plt.title('Students Attending Extra Classes')
plt.show()

# STEP 4.2: Bivariate Analysis
sns.boxplot(x='gender', y='Final_Grade', hue='gender', data=df, palette='Set2', legend=False)
plt.title('Final Grade by Gender')
plt.show()

# STEP 4.3: Multivariate Analysis
plt.figure(figsize=(8,5))
numerical_df = df.select_dtypes(include=['number'])
sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

# Step 5: Model Building (ML Models)
# Step 5.1: Linear Regression
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


# Create and train the model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Predictions
y_pred_lr = lr_model.predict(X_test)

# Evaluation
mse = mean_squared_error(y_test, y_pred_lr)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred_lr)

print("Linear Regression:")
print("MSE:", mse)
print("RMSE:", rmse)
print("R² Score:", r2)

# Step 5.2: Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Training the Decision Tree Regressor
dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)

# Predicting the values
y_pred_dt = dt.predict(X_test)

# Evaluation metrics
mse_dt = mean_squared_error(y_test, y_pred_dt)
rmse_dt = np.sqrt(mse_dt)
r2_dt = r2_score(y_test, y_pred_dt)

# Displaying the results
print("Decision Tree Regressor Evaluation:")
print("MSE:", mse_dt)
print("RMSE:", rmse_dt)
print("R² Score:", r2_dt)

# Step 6: XGBoost
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Train XGBoost model
xgb = XGBRegressor()
xgb.fit(X_train, y_train)

y_pred_xgb = xgb.predict(X_test)

mse_xgb = mean_squared_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mse_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print("XGBoost Regressor Evaluation:")
print("MSE:", mse_xgb)
print("RMSE:", rmse_xgb)
print("R² Score:", r2_xgb)

#  Step 7: Dimensionality Reduction using PCA
# PCA for Dimensionality Reduction
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print("Original shape:", X_scaled.shape)
print("Reduced shape:", X_pca.shape)

print("Explained Variance Ratio:", pca.explained_variance_ratio_)

# Visualizing PCA Result
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y, palette='viridis')
plt.title('PCA - 2D Projection of Student Data')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.show()

# Step 8: Regularization (Ridge and Lasso)

from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Features and target
X = df.select_dtypes(include=['int64', 'float64']).drop(columns=['math_score'])  # Predict math_score
y = df['math_score']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ridge Regression
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
ridge_preds = ridge.predict(X_test)

# Lasso Regression
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
lasso_preds = lasso.predict(X_test)

print("Ridge RMSE:", np.sqrt(mean_squared_error(y_test, ridge_preds)))
print("Lasso RMSE:", np.sqrt(mean_squared_error(y_test, lasso_preds)))

coef = pd.Series(ridge.coef_, index=X_train.columns)
coef = coef.sort_values(key=abs, ascending=False)

plt.figure(figsize=(10,5))
coef.plot(kind='bar', color='skyblue')
plt.title("Feature Importance (Ridge Regression)")
plt.ylabel("Coefficient Value")
plt.grid(True)
plt.tight_layout()
plt.show()

# Step 9: Confusion Matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import LabelEncoder

# Encode categorical target
le = LabelEncoder()
df['sports_encoded'] = le.fit_transform(df['sports_participation'])

# Classification: Predicting sports_participation
X = df.select_dtypes(include=['int64', 'float64']).drop(columns=['sports_encoded'])
y = df['sports_encoded']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = RandomForestClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

#  Step 10: Gradient Descent Implementation (Manual for y = mx + b)
# Simple gradient descent to predict math_score from reading_score

X = df['reading_score'].values
y = df['math_score'].values

m, b = 0, 0
L = 0.0001
epochs = 1000

for i in range(epochs):
    y_pred = m * X + b
    error = y - y_pred
    m_grad = -2 * (X * error).mean()
    b_grad = -2 * error.mean()
    m -= L * m_grad
    b -= L * b_grad

print(f"Slope (m): {m:.4f}, Intercept (b): {b:.4f}")

# Step 11: Model Evaluation (RMSE, MSE, R²)
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge


X = df.select_dtypes(include=['int64', 'float64']).drop(columns=['math_score'])
y = df['math_score']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Refit model
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Predict and evaluate
preds = ridge.predict(X_test)

print("MSE:", mean_squared_error(y_test, preds))
print("RMSE:", np.sqrt(mean_squared_error(y_test, preds)))
print("R² Score:", r2_score(y_test, preds))

# Step 12: Cross Validation
from sklearn.model_selection import cross_val_score

scores = cross_val_score(ridge, X, y, cv=5, scoring='r2')
print("Cross-Validation R² Scores:", scores)
print("Average R²:", scores.mean())

# Step 13: Correlation & Covariance
print("Correlation Matrix:")
print(df.corr(numeric_only=True))

print("\nCovariance Matrix:")
print(df.cov(numeric_only=True))